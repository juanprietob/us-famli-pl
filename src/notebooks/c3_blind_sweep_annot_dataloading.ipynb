{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nrrd\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/us-famli-pl/src/\")\n",
    "\n",
    "from loaders import ultrasound_dataset as usd\n",
    "from nets import classification as usn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Datasets and loaders\n",
    "# group.add_argument('--mount_point', type=str, default=\"./\", help=\"Mount point for the data\")\n",
    "# group.add_argument('--csv_train', type=str, required=True, help=\"Training data csv file path\")\n",
    "# group.add_argument('--csv_valid', type=str, required=True, help=\"Validation data csv file path\")\n",
    "# group.add_argument('--csv_test', type=str, required=True, help=\"Test data csv file path\")\n",
    "# group.add_argument('--img_column', type=str, default=\"file_path\")\n",
    "# group.add_argument('--tag_column', type=str, default=\"tag\")\n",
    "# group.add_argument('--frame_column', type=str, default=\"frame_index\")\n",
    "# group.add_argument('--frame_label', type=str, default=\"annotation_label\")\n",
    "# group.add_argument('--id_column', type=str, default=\"annotation_id\")\n",
    "# group.add_argument('--batch_size', type=int, default=4, help=\"Batch size for the train dataloaders\")\n",
    "# group.add_argument('--num_frames', type=int, default=64, help=\"Number of frames to sample from each cine\")\n",
    "# group.add_argument('--num_workers', type=int, default=1)\n",
    "# group.add_argument('--prefetch_factor', type=int, default=2)\n",
    "# group.add_argument('--drop_last', type=int, default=False)\n",
    "\n",
    "\n",
    "args_dict = {\n",
    "    \"mount_point\": \"/mnt/raid/C1_ML_Analysis/\",\n",
    "    \"csv_train\": \"/mnt/raid/C1_ML_Analysis/CSV_files/c3_blindsweep_annotation_labels_merged_train_train.csv\",\n",
    "    \"csv_valid\": \"/mnt/raid/C1_ML_Analysis/CSV_files/c3_blindsweep_annotation_labels_merged_train_test.csv\",\n",
    "    \"csv_test\": \"/mnt/raid/C1_ML_Analysis/CSV_files/c3_blindsweep_annotation_labels_merged_test.csv\",\n",
    "    \"img_column\": \"file_path\",\n",
    "    \"tag_column\": \"tag\",\n",
    "    \"frame_column\": \"frame_index\",\n",
    "    \"frame_label\": \"annotation_label\",\n",
    "    \"id_column\": \"annotation_id\",\n",
    "    \"batch_size\": 1,\n",
    "    \"num_frames\": 64,\n",
    "    \"num_workers\": 1,\n",
    "    \"prefetch_factor\": 1,\n",
    "    \"drop_last\": False\n",
    "}\n",
    "\n",
    "batch_size = 2\n",
    "dm = usd.USAnnotatedBlindSweepDataModule(\n",
    "        **args_dict\n",
    "    )\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = dm.train_dataloader()\n",
    "valid_dl = dm.val_dataloader()\n",
    "test_dl = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_tensor_grid(tensor, nrow=4, title=\"Image Grid\", scale=1.0):\n",
    "    \"\"\"\n",
    "    Uses torchvision's make_grid to arrange images into a grid, then displays it with Plotly.\n",
    "\n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape [B, N, C, T, H, W]\n",
    "        nrow: Number of images per row in the grid\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    B, N, C, T, H, W = tensor.shape\n",
    "    tensor = tensor.permute(0,1,3,2,4,5)  # [B, N, T, C, H, W]\n",
    "\n",
    "    # Flatten batch and image dimensions\n",
    "    tensor = tensor.view(B * N * T, C, H, W)\n",
    "\n",
    "    # Create image grid\n",
    "    grid = make_grid(tensor, nrow=nrow, padding=2, normalize=True, scale_each=True)\n",
    "\n",
    "    # Convert to numpy for Plotly\n",
    "    grid_np = grid.permute(1,2,0).cpu().numpy()  # shape [H, W] since it's grayscale\n",
    "\n",
    "    # Determine scaled dimensions\n",
    "    height, width, C = grid_np.shape\n",
    "    display_height = int(height * scale)\n",
    "    display_width = int(width * scale)\n",
    "\n",
    "    # Plot with Plotly\n",
    "    fig = px.imshow(grid_np)\n",
    "    fig.update_layout(\n",
    "        title=title,        \n",
    "        width=display_width,\n",
    "        height=display_height\n",
    "    )\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor_grid_batch(tensor, nrow=4, title=\"Image Grid\", scale=1.0):\n",
    "    \"\"\"\n",
    "    Uses torchvision's make_grid to arrange images into a grid, then displays it with Plotly.\n",
    "\n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape [B, C, H, W]\n",
    "        nrow: Number of images per row in the grid\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    B, C, H, W = tensor.shape\n",
    "\n",
    "    # Create image grid\n",
    "    grid = make_grid(tensor, nrow=nrow, padding=2, normalize=True, scale_each=True)\n",
    "\n",
    "    # Convert to numpy for Plotly\n",
    "    grid_np = grid.permute(1,2,0).cpu().numpy()  # shape [H, W] since it's grayscale\n",
    "\n",
    "    # Determine scaled dimensions\n",
    "    height, width, C = grid_np.shape\n",
    "    display_height = int(height * scale)\n",
    "    display_width = int(width * scale)\n",
    "\n",
    "    # Plot with Plotly\n",
    "    fig = px.imshow(grid_np)\n",
    "    fig.update_layout(\n",
    "        title=title,        \n",
    "        width=display_width,\n",
    "        height=display_height\n",
    "    )\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "\n",
    "# plot_tensor_grid(batch[0], nrow=4, title='Image Grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dl:\n",
    "#     img, labels, tags = batch\n",
    "\n",
    "# for batch in valid_dl:\n",
    "#     img, labels, tags = batch\n",
    "\n",
    "# for batch in test_dl:\n",
    "#     img, labels, tags = batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group = parent_parser.add_argument_group(\"Fetal Anatomy time aware classification Model\")\n",
    "\n",
    "# group.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "# group.add_argument(\"--betas\", type=tuple, default=(0.9, 0.999), help='Betas for Adam optimizer')\n",
    "# group.add_argument('--weight_decay', help='Weight decay for optimizer', type=float, default=1e-5)\n",
    "\n",
    "# # Image Encoder parameters \n",
    "# group.add_argument(\"--spatial_dims\", type=int, default=2, help='Spatial dimensions for the encoder')\n",
    "# group.add_argument(\"--in_channels\", type=int, default=3, help='Input channels for encoder')\n",
    "# group.add_argument(\"--features\", type=int, default=1280, help='Number of output features for the encoder')\n",
    "# group.add_argument(\"--time_dim_train\", type=int, nargs=\"+\", default=(16, 128), help='Range of time dimensions for training')\n",
    "# group.add_argument(\"--n_chunks_e\", type=int, default=2, help='Number of chunks in the encoder stage to reduce memory usage')\n",
    "# group.add_argument(\"--n_chunks\", type=int, default=16, help='Number of outputs in the time dimension, this will determine the first dimension of the 2D positional encoding')\n",
    "# group.add_argument(\"--num_heads\", type=int, default=8, help='Number of heads for multi_head attention')\n",
    "\n",
    "# group.add_argument(\"--embed_dim\", type=int, default=128, help='Embedding dimension')        \n",
    "# group.add_argument(\"--dropout\", type=float, default=0.1, help='Dropout rate')\n",
    "# group.add_argument(\"--tags\", type=int, default=17, help='Number of sweep tags for the sequences, this will determine the second dimension of the 2D positional encoding')\n",
    "\n",
    "# group.add_argument(\"--num_classes\", type=int, default=3, help='Number of output classes for the model')        \n",
    "# group.add_argument(\"--class_weights\", nargs=\"+\", default=None, type=float, help='Class weights for the loss function')\n",
    "\n",
    "n_tags = 16\n",
    "\n",
    "args_model = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"in_channels\": 3,\n",
    "    \"features\": 1280,\n",
    "    \"n_chunks_e\": 2,\n",
    "    \"n_chunks\": 16,\n",
    "    \"num_heads\": 8,\n",
    "    \"embed_dim\": 128,\n",
    "    \"dropout\": 0.25,\n",
    "    \"tags\": n_tags,\n",
    "    \"num_classes\": 5,\n",
    "    \"class_weights\": [0.1, 0.7, 4, 30, 200]\n",
    "}\n",
    "\n",
    "model = usn.FlyToClassification(**args_model).cuda()\n",
    "\n",
    "X, sweep_tags, Y = batch\n",
    "X_hat = model(X.cuda(), sweep_tags.cuda())\n",
    "# x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "df_train = pd.read_csv(args_dict['csv_train'])\n",
    "\n",
    "cl = df_train['annotation_label'].apply(lambda x: dm.train_ds.frame_labels_dict[x])\n",
    "cl.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_r = list(cl)\n",
    "cl_r += [0] * (2*len(cl_r))\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(cl_r), y=cl_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv(os.path.join(args_dict['mount_point'], 'CSV_files/c3_blindsweep_annotation_labels_merged.csv'))\n",
    "df_merged['annotation_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Datasets and loaders\n",
    "# group.add_argument('--mount_point', type=str, default=\"./\", help=\"Mount point for the data\")\n",
    "# group.add_argument('--csv_train', type=str, required=True, help=\"Training data csv file path\")\n",
    "# group.add_argument('--csv_valid', type=str, required=True, help=\"Validation data csv file path\")\n",
    "# group.add_argument('--csv_test', type=str, required=True, help=\"Test data csv file path\")\n",
    "# group.add_argument('--img_column', type=str, default=\"file_path\")\n",
    "# group.add_argument('--tag_column', type=str, default=\"tag\")\n",
    "# group.add_argument('--frame_column', type=str, default=\"frame_index\")\n",
    "# group.add_argument('--frame_label', type=str, default=\"annotation_label\")\n",
    "# group.add_argument('--id_column', type=str, default=\"annotation_id\")\n",
    "# group.add_argument('--batch_size', type=int, default=4, help=\"Batch size for the train dataloaders\")\n",
    "# group.add_argument('--num_frames', type=int, default=64, help=\"Number of frames to sample from each cine\")\n",
    "# group.add_argument('--num_workers', type=int, default=1)\n",
    "# group.add_argument('--prefetch_factor', type=int, default=2)\n",
    "# group.add_argument('--drop_last', type=int, default=False)\n",
    "\n",
    "\n",
    "# args_dict = {\n",
    "#     \"mount_point\": \"/mnt/raid/C1_ML_Analysis/\",\n",
    "#     \"csv_train\": \"/mnt/raid/C1_ML_Analysis/CSV_files/c3_blindsweep_annotation_labels_merged_train_train.csv\",\n",
    "#     \"csv_valid\": \"/mnt/raid/C1_ML_Analysis/CSV_files/c3_blindsweep_annotation_labels_merged_train_test.csv\",\n",
    "#     \"csv_test\": \"/mnt/raid/C1_ML_Analysis/CSV_files/c3_blindsweep_annotation_labels_merged_test.csv\",\n",
    "#     \"img_column\": \"file_path\",    \n",
    "#     \"frame_column\": \"frame_index\",\n",
    "#     \"frame_label\": \"annotation_label\",\n",
    "#     \"id_column\": \"annotation_id\",\n",
    "#     \"batch_size\": 2,\n",
    "#     \"num_frames\": 64,\n",
    "#     \"num_workers\": 1,\n",
    "#     \"prefetch_factor\": 1,\n",
    "#     \"use_extra_ac\": 0,\n",
    "#     \"drop_last\": False\n",
    "# }\n",
    "\n",
    "# batch_size = 2\n",
    "# dm = usd.USAnnotatedBlindSweep2DFramesDataModule(\n",
    "#         **args_dict\n",
    "#     )\n",
    "# dm.setup()\n",
    "\n",
    "\n",
    "# batch = next(iter(dm.train_dataloader()))\n",
    "# batch[0].shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dm.train_dataloader():\n",
    "#     img, labels = batch\n",
    "\n",
    "# for batch in dm.val_dataloader():\n",
    "#     img, labels = batch\n",
    "\n",
    "# for batch in dm.test_dataloader():\n",
    "#     img, labels = batch\n",
    "#     img, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseLevelTransform:\n",
    "    \"\"\"\n",
    "    Adds random Gaussian noise to `x`, scaled per element along a chosen dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=0, min_scale=0.0, max_scale=0.5, noise_std=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Dimension along which to vary noise level (e.g. 0=batch, 1=frames, 2=channels).\n",
    "            min_scale (float): Minimum blending factor for noise (0=no noise).\n",
    "            max_scale (float): Maximum blending factor for noise (1=full noise).\n",
    "            noise_std (float): Standard deviation of Gaussian noise.\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def __call__(self, x):\n",
    "        noise = torch.randn_like(x) * self.noise_std\n",
    "\n",
    "        # Create broadcastable noise scaling\n",
    "        scale_shape = [1] * x.ndim\n",
    "        scale_shape[self.dim] = x.shape[self.dim]\n",
    "        scale = torch.empty(scale_shape, device=x.device).uniform_(self.min_scale, self.max_scale)\n",
    "\n",
    "        return (1.0 - scale) * x + scale * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_tensor_grid_batch(NoiseLevelTransform()(batch[0]), nrow=4, title='Image Grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = \"/mnt/raid/C1_ML_Analysis/\"\n",
    "df_c3 = pd.read_csv(os.path.join(mount_point,'CSV_files/c3_instance_analysis_dataset.csv'))\n",
    "df_c3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c3.query('cine == 1')['tag'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_c3_flyto = df_c3.query('split == \"train\" and cine == 1')\n",
    "\n",
    "# df_c3_flyto = df_c3_flyto[df_c3_flyto['tag'].isin(['AC', 'BPD', 'FL', 'TCD', 'HL', 'CRL'])]\n",
    "# df_c3_flyto_dcm = df_c3_flyto[df_c3_flyto['file_path'].str.contains('.dcm', na=False)]\n",
    "# df_c3_flyto_mp4 = df_c3_flyto[~df_c3_flyto['file_path'].str.contains('.dcm', na=False)]\n",
    "\n",
    "# df_c3_flyto_dcm.to_csv(os.path.join(mount_point,'CSV_files/c3_instance_analysis_dataset_flyto_dcm.csv'), index=False)\n",
    "\n",
    "# df_c3_flyto_mp4['pixel_spacing_x_mp4'] = df_c3_flyto_mp4['pixel_spacing_x_mp4']*10\n",
    "# df_c3_flyto_mp4['pixel_spacing_y_mp4'] = df_c3_flyto_mp4['pixel_spacing_y_mp4']*10\n",
    "# df_c3_flyto_mp4.to_csv(os.path.join(mount_point,'CSV_files/c3_instance_analysis_dataset_flyto_mp4.csv'), index=False)\n",
    "\n",
    "# df_c3_flyto['file_path'] = df_c3_flyto['file_path'].str.replace('Groups/FAMLI/Restricted_access_data/Ultrasound/', '').str.replace('Groups/FAMLI/Deidentified_data/Ultrasound/', '').str.replace(f'Dataset_C3', f'Dataset_C3_masked_resampled_256_spc075').str.replace('.mp4', '.nrrd').str.replace('.dcm', '.nrrd')\n",
    "\n",
    "# print(f\"Total FlyTo train samples before filtering: {len(df_c3_flyto)}\")\n",
    "# df_c3_flyto_exists = df_c3_flyto[df_c3_flyto['file_path'].apply(lambda x: os.path.exists(os.path.join(mount_point, str(x))))]\n",
    "# print(f\"Total FlyTo train samples after filtering: {len(df_c3_flyto_exists)}\")\n",
    "\n",
    "# df_c3_flyto_exists.to_csv(os.path.join(mount_point,'CSV_files/c3_instance_analysis_dataset_flyto_train_masked_resampled_256_spc075.csv'), index=False)\n",
    "\n",
    "\n",
    "# df_c3_flyto['file_path'].to_csv(os.path.join(mount_point,'CSV_files/c3_instance_analysis_dataset_flyto_train_masked_resampled_filepaths.txt'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
